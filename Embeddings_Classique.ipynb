{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ \n",
    "\"Je suis heureux d'être ici.\", \n",
    "\"L'apprentissage automatique est fascinant.\", \n",
    "\"Les modèles de langage sont importants pour le NLP.\", \n",
    "\"Le NLP se concentre sur le traitement du langage naturel.\", \n",
    "\"Les embeddings de mots capturent le sens des mots.\" \n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implémentation des techniques classiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mot: ici, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: automatique, Vecteur binaire: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: capturent, Vecteur binaire: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: modèles, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: le, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: lapprentissage, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: suis, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Mot: pour, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Mot: sont, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "Mot: importants, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: du, Vecteur binaire: [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: fascinant, Vecteur binaire: [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: langage, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: concentre, Vecteur binaire: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: dêtre, Vecteur binaire: [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: sens, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Mot: de, Vecteur binaire: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: traitement, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Mot: sur, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Mot: heureux, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: naturel, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Mot: nlp, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Mot: embeddings, Vecteur binaire: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: les, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: mots, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Mot: est, Vecteur binaire: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: je, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: des, Vecteur binaire: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mot: se, Vecteur binaire: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_docs = []\n",
    "for doc in documents:\n",
    "    # Conversion en minuscules\n",
    "    doc = doc.lower()\n",
    "    # Suppression de la ponctuation\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    # Tokenization (division en mots)\n",
    "    tokens = doc.split()\n",
    "    preprocessed_docs.extend(tokens)\n",
    "\n",
    "# Création d'un dictionnaire pour chaque mot unique\n",
    "unique_words = list(set(preprocessed_docs))\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# Attribution d'un vecteur binaire unique à chaque mot\n",
    "label_binarizer = LabelBinarizer()\n",
    "binary_vectors = label_binarizer.fit_transform(unique_words)\n",
    "\n",
    "# Affichage des mots uniques avec leur vecteur binaire\n",
    "for word, vector in zip(unique_words, binary_vectors):\n",
    "    print(f\"Mot: {word}, Vecteur binaire: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(docs): \n",
    "    vocab = set(word for doc in docs for word in doc.split()) \n",
    "    vocab = {word: i for i, word in enumerate(vocab)} \n",
    "     \n",
    "    one_hot_vectors = [] \n",
    "    for doc in docs: \n",
    "        vector = [0] * len(vocab) \n",
    "        for word in doc.split(): \n",
    "            if word in vocab: \n",
    "                vector[vocab[word]] = 1 \n",
    "        one_hot_vectors.append(vector) \n",
    "    return one_hot_vectors, vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire: {'ici': 0, 'automatique': 1, 'capturent': 2, 'modèles': 3, 'le': 4, 'lapprentissage': 5, 'suis': 6, 'pour': 7, 'sont': 8, 'importants': 9, 'du': 10, 'fascinant': 11, 'langage': 12, 'concentre': 13, 'dêtre': 14, 'sens': 15, 'de': 16, 'traitement': 17, 'sur': 18, 'heureux': 19, 'naturel': 20, 'nlp': 21, 'embeddings': 22, 'les': 23, 'mots': 24, 'est': 25, 'je': 26, 'des': 27, 'se': 28}\n",
      "One-Hot Encodings: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_vectors, vocab = one_hot_encoding(preprocessed_docs) \n",
    "print(\"Vocabulaire:\", vocab) \n",
    "print(\"One-Hot Encodings:\", one_hot_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Liste des stopwords en français\n",
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation et les chiffres\n",
    "    text = re.sub(r'[^a-zéàèùâêîôûëïöüç]', ' ', text) \n",
    "    tokens = text.split()\n",
    "    # Suppression des stopwords et stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    # Rejoindre les mots traités pour reformer le texte\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer le prétraitement à chaque document\n",
    "documents_preprocessed = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Initialiser le CountVectorizer pour créer un modèle BOW\n",
    "vectorizer = CountVectorizer(binary=True)  # On choisit un modèle binaire pour le vecteur présence/absence\n",
    "X = vectorizer.fit_transform(documents_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire (mots uniques) :\n",
      "['apprentissag' 'automatiqu' 'captur' 'concentr' 'embed' 'fascin'\n",
      " 'heureux' 'ici' 'import' 'langag' 'modèl' 'mot' 'naturel' 'nlp' 'sen'\n",
      " 'traitement' 'être']\n"
     ]
    }
   ],
   "source": [
    "# Affichage du vocabulaire (dictionnaire des mots uniques)\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulaire (mots uniques) :\")\n",
    "print(vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vecteurs binaires pour chaque document :\n",
      "[[0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Affichage des vecteurs binaires (matrice de présence/absence)\n",
    "print(\"\\nVecteurs binaires pour chaque document :\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire: ['apprentissag' 'automatiqu' 'captur' 'concentr' 'embed' 'fascin'\n",
      " 'heureux' 'ici' 'import' 'langag' 'modèl' 'mot' 'naturel' 'nlp' 'sen'\n",
      " 'traitement' 'être']\n",
      "Matrice BoW:\n",
      " [[0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 2 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer() \n",
    "bow_matrix = vectorizer.fit_transform(documents_preprocessed) \n",
    "print(\"Vocabulaire:\", vectorizer.get_feature_names_out()) \n",
    "print(\"Matrice BoW:\\n\", bow_matrix.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Term Frequency - Inverse Document Frequency (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire TF-IDF: ['apprentissag' 'automatiqu' 'captur' 'concentr' 'embed' 'fascin'\n",
      " 'heureux' 'ici' 'import' 'langag' 'modèl' 'mot' 'naturel' 'nlp' 'sen'\n",
      " 'traitement' 'être']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents_preprocessed)\n",
    "print(\"Vocabulaire TF-IDF:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice TF-IDF:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.57735027 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.57735027]\n",
      " [0.57735027 0.57735027 0.         0.         0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.55032913 0.44400208 0.55032913 0.\n",
      "  0.         0.44400208 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.48214012 0.         0.\n",
      "  0.         0.         0.         0.38898761 0.         0.\n",
      "  0.48214012 0.38898761 0.         0.48214012 0.        ]\n",
      " [0.         0.         0.37796447 0.         0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.75592895\n",
      "  0.         0.         0.37796447 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrice TF-IDF:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire BoW: ['apprentissag' 'automatiqu' 'captur' 'concentr' 'embed' 'fascin'\n",
      " 'heureux' 'ici' 'import' 'langag' 'modèl' 'mot' 'naturel' 'nlp' 'sen'\n",
      " 'traitement' 'être']\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le CountVectorizer pour créer un modèle BOW\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents_preprocessed)\n",
    "\n",
    "# Affichage du vocabulaire (dictionnaire des mots uniques)\n",
    "print(\"Vocabulaire BoW:\", bow_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice BoW:\n",
      " [[0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 2 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Affichage des vecteurs de fréquence des mots pour chaque document\n",
    "print(\"Matrice BoW:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots avec TF-IDF égal à 0:\n"
     ]
    }
   ],
   "source": [
    "# --- Analyse des différences ---\n",
    "# Vérifier les mots avec des vecteurs TF-IDF égal à 0\n",
    "print(\"Mots avec TF-IDF égal à 0:\")\n",
    "for word, index in tfidf_vectorizer.vocabulary_.items():\n",
    "    if all(tfidf_matrix[:, index].toarray() == 0):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire: ['apprentissag' 'automatiqu' 'captur' 'concentr' 'embed' 'fascin'\n",
      " 'heureux' 'ici' 'import' 'langag' 'modèl' 'mot' 'naturel' 'nlp' 'sen'\n",
      " 'traitement' 'être']\n",
      "Matrice TF-IDF:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.57735027 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.57735027]\n",
      " [0.57735027 0.57735027 0.         0.         0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.55032913 0.44400208 0.55032913 0.\n",
      "  0.         0.44400208 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.48214012 0.         0.\n",
      "  0.         0.         0.         0.38898761 0.         0.\n",
      "  0.48214012 0.38898761 0.         0.48214012 0.        ]\n",
      " [0.         0.         0.37796447 0.         0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.75592895\n",
      "  0.         0.         0.37796447 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents_preprocessed) \n",
    "print(\"Vocabulaire:\", tfidf_vectorizer.get_feature_names_out()) \n",
    "print(\"Matrice TF-IDF:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams Vocabulaire: ['apprentissag automatiqu' 'automatiqu fascin' 'captur sen'\n",
      " 'concentr traitement' 'embed mot' 'heureux être' 'import nlp'\n",
      " 'langag import' 'langag naturel' 'modèl langag' 'mot captur'\n",
      " 'nlp concentr' 'sen mot' 'traitement langag' 'être ici']\n",
      "Matrice Bigram:\n",
      " [[0 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2)) \n",
    "bigram_matrix = bigram_vectorizer.fit_transform(documents_preprocessed) \n",
    "print(\"Bigrams Vocabulaire:\", bigram_vectorizer.get_feature_names_out()) \n",
    "print(\"Matrice Bigram:\\n\", bigram_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams Vocabulaire: ['apprentissag automatiqu fascin' 'captur sen mot'\n",
      " 'concentr traitement langag' 'embed mot captur' 'heureux être ici'\n",
      " 'langag import nlp' 'modèl langag import' 'mot captur sen'\n",
      " 'nlp concentr traitement' 'traitement langag naturel']\n",
      "Matrice Trigram:\n",
      " [[0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 1 1]\n",
      " [0 1 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3)) \n",
    "trigram_matrix = trigram_vectorizer.fit_transform(documents_preprocessed) \n",
    "print(\"Trigrams Vocabulaire:\", trigram_vectorizer.get_feature_names_out()) \n",
    "print(\"Matrice Trigram:\\n\", trigram_matrix.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse des Résultats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la structure des matrices obtenues avec chaque technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) :\n",
    "La matrice est clairsemée, avec une dimension de 5×n (où n est le nombre total de mots uniques dans les documents). Chaque ligne représente un document, et chaque colonne une occurrence des mots. Cette structure ne capture pas le contexte sémantique.\n",
    "\n",
    "### TF-IDF :\n",
    "Semblable à BoW mais avec des poids attribués aux termes en fonction de leur fréquence dans un document et dans le corpus entier. Les valeurs non nulles indiquent les mots discriminants. La matrice reste clairsemée et ne prend pas en compte l'ordre des mots ni le contexte.\n",
    "\n",
    "### Word Embeddings (Word2Vec/GloVe) :\n",
    "Les matrices générées ici sont denses, chaque mot étant représenté par un vecteur de dimension fixe (ex. 100 ou 300 dimensions). Ces vecteurs capturent les similarités sémantiques : des mots proches dans le sens auront des vecteurs proches dans l’espace vectoriel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des tailles des matrices et interprétation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW et TF-IDF :\n",
    "La dimension dépend directement du vocabulaire, ici un corpus restreint génère une matrice de petite taille mais très clairsemée. L’interprétation est simple, mais ces représentations souffrent d’une perte de sémantique et sont inefficaces pour les corpus volumineux.\n",
    "\n",
    "### Word2Vec/GloVe :\n",
    "Les dimensions des vecteurs sont constantes (fixées lors de l’entraînement ou via des modèles pré-entraînés comme GloVe). Dans ce cas, la matrice est denses et de taille N×d, où N est le nombre de mots uniques et d la dimension des vecteurs (ex. 100 ou 300). Cela améliore la généralisation et la capture de relations contextuelles et sémantiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations du One-Hot Encoding et du BoW :\n",
    "\n",
    "One-Hot Encoding :\n",
    "Matrices très clairsemées pour les grands vocabulaires.\n",
    "Perte totale du contexte et des relations sémantiques entre les mots.\n",
    "BoW :\n",
    "Ignore l'ordre des mots et le contexte.\n",
    "Sensible aux mots fréquents non informatifs (ex. articles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Améliorations apportées par le TF-IDF :\n",
    "\n",
    "Réduit l’importance des mots courants non informatifs en pondérant leur fréquence inversement à leur présence dans le corpus.\n",
    "Met en valeur les mots rares et discriminants pour chaque document, améliorant ainsi la qualité des représentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apport des n-grams :\n",
    "\n",
    "Capturent les séquences de mots (ex. bigrams, trigrams) pour inclure une partie du contexte.\n",
    "Révèlent des relations syntaxiques et sémantiques non accessibles avec des mots isolés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extension du Corpus avec NLTK et Analyse des Temps d'Exécution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('movie_reviews') \n",
    "from nltk.corpus import movie_reviews \n",
    "# Charger les phrases du corpus \n",
    "corpus = [\" \".join(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Conversion en minuscules\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Suppression des stop words et de la ponctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Appliquer le pré-traitement sur le corpus\n",
    "processed_corpus = [preprocess_text(text) for text in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoindre les tokens traités en textes\n",
    "processed_texts = [\" \".join(text) for text in processed_corpus]\n",
    "\n",
    "# Initialiser CountVectorizer pour One-Hot Encoding\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "one_hot_matrix = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Convertir en tableau dense pour visualisation\n",
    "one_hot_array = one_hot_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Convertir en tableau dense pour visualisation\n",
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Convertir en tableau dense pour visualisation\n",
    "bow_array = bow_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Convertir en tableau dense pour visualisation\n",
    "ngram_array = ngram_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions d'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 :\n",
    "Méthode la plus rapide : One-Hot Encoding, car elle crée un vecteur binaire sans calcul complexe.\n",
    "Méthode la plus lente : N-gram.\n",
    "\n",
    "Question 2 :\n",
    "La taille du corpus augmente la complexité des méthodes. Le One-Hot Encoding et le BoW deviennent plus lents à mesure que le vocabulaire augmente. TF-IDF et N-grams sont également plus lents, particulièrement avec des grands corpus, en raison du calcul des fréquences.\n",
    "\n",
    "Question 3 :\n",
    "TF-IDF et N-grams sont particulièrement affectés par l'augmentation de la taille du corpus, car ces méthodes nécessitent des calculs plus complexes (comptage de fréquence et pondération) et des mémoires plus importantes pour stocker les informations.\n",
    "\n",
    "Question 4 :\n",
    "Avec un grand corpus, les vecteurs obtenus devraient être plus dispersés et mieux capturer les relations entre les mots. On peut observer des améliorations dans les représentations, mais aussi une augmentation de la sparsité et des variations dans les poids (pour TF-IDF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
