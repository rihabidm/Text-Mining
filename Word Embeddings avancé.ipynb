{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Préparation et chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import numpy as np \n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import FastText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ \n",
    "\"Je suis heureux d'être ici.\", \n",
    "\"L'apprentissage automatique est fascinant.\", \n",
    "\"Les modèles de langage sont importants pour le NLP.\", \n",
    "\"Le NLP se concentre sur le traitement du langage naturel.\", \n",
    "\"Les embeddings de mots capturent le sens des mots.\" \n",
    "] \n",
    "# Prétraitement : Tokenisation de chaque phrase \n",
    "tokenized_docs = [doc.lower().split() for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implémentation des techniques statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    # Supprimer la ponctuation et convertir en minuscules\n",
    "    doc = doc.lower()\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenisation\n",
    "    tokens = doc.split()\n",
    "    # Supprimer les mots vides\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokenized_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding CBOW pour 'heureux': [ 1.30016683e-03 -9.80430283e-03  4.58776252e-03 -5.38222783e-04\n",
      "  6.33209571e-03  1.78347470e-03 -3.12979822e-03  7.75997294e-03\n",
      "  1.55466562e-03  5.52093989e-05 -4.61295387e-03 -8.45352374e-03\n",
      " -7.76683213e-03  8.67050979e-03 -8.92496016e-03  9.03471559e-03\n",
      " -9.28101782e-03 -2.76756298e-04 -1.90704700e-03 -8.93114600e-03\n",
      "  8.63005966e-03  6.77781366e-03  3.01943906e-03  4.83345287e-03\n",
      "  1.12190246e-04  9.42468084e-03  7.02128746e-03 -9.85372625e-03\n",
      " -4.43322072e-03 -1.29011157e-03  3.04772262e-03 -4.32395237e-03\n",
      "  1.44916656e-03 -7.84589909e-03  2.77807354e-03  4.70269192e-03\n",
      "  4.93731257e-03 -3.17570218e-03 -8.42704065e-03 -9.22061782e-03\n",
      " -7.22899451e-04 -7.32746487e-03 -6.81496272e-03  6.12000562e-03\n",
      "  7.17230327e-03  2.11741915e-03 -7.89940078e-03 -5.69898821e-03\n",
      "  8.05184525e-03  3.92084382e-03 -5.24047017e-03 -7.39190448e-03\n",
      "  7.71554711e-04  3.46375466e-03  2.07919348e-03  3.10080405e-03\n",
      " -5.62050007e-03 -9.88948625e-03 -7.02083716e-03  2.30308768e-04\n",
      "  4.61867917e-03  4.52630781e-03  1.87981245e-03  5.17067453e-03\n",
      " -1.05360748e-04  4.11416637e-03 -9.12324060e-03  7.70091172e-03\n",
      "  6.14747405e-03  5.12415636e-03  7.20666908e-03  8.43979698e-03\n",
      "  7.38695846e-04 -1.70386070e-03  5.18628338e-04 -9.31678060e-03\n",
      "  8.40621442e-03 -6.37993217e-03  8.42784252e-03 -4.24435502e-03\n",
      "  6.46842702e-04 -9.16416850e-03 -9.55856778e-03 -7.83681031e-03\n",
      " -7.73105631e-03  3.75581993e-04 -7.22646248e-03 -4.95021325e-03\n",
      " -5.27170673e-03 -4.28929785e-03  7.01231137e-03  4.82938997e-03\n",
      "  8.68277065e-03  7.09359162e-03 -5.69440611e-03  7.24079600e-03\n",
      " -9.29490291e-03 -2.58756871e-03 -7.75716640e-03  4.19260142e-03]\n"
     ]
    }
   ],
   "source": [
    "# CBOW Model \n",
    "cbow_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, sg=0) \n",
    "cbow_vector = cbow_model.wv['heureux']  # Exemple d'embedding pour un mot \n",
    "print(\"Embedding CBOW pour 'heureux':\", cbow_vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vecteur pour \"heureux\" est une représentation d'un mot dans un espace vectoriel de 100 dimensions, capturant ses relations sémantiques et contextuelles dans le corpus d'entraînement. Les valeurs du vecteur sont des coefficients appris par le modèle et ne portent pas de signification directe, mais elles permettent de calculer la similarité entre ce mot et d'autres mots. Par exemple, des mots ayant une signification similaire à \"heureux\" (comme \"content\" ou \"joyeux\") seront représentés par des vecteurs proches dans cet espace.\n",
    "\n",
    "Le temps d'exécution de **0.0s** est très rapide en raison de la petite taille du corpus (seulement 5 phrases), ce qui rend l'entraînement du modèle très rapide. Sur un plus grand corpus, le temps d'exécution serait plus long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Skip-gram pour 'heureux': [ 1.30016683e-03 -9.80430283e-03  4.58776252e-03 -5.38222783e-04\n",
      "  6.33209571e-03  1.78347470e-03 -3.12979822e-03  7.75997294e-03\n",
      "  1.55466562e-03  5.52093989e-05 -4.61295387e-03 -8.45352374e-03\n",
      " -7.76683213e-03  8.67050979e-03 -8.92496016e-03  9.03471559e-03\n",
      " -9.28101782e-03 -2.76756298e-04 -1.90704700e-03 -8.93114600e-03\n",
      "  8.63005966e-03  6.77781366e-03  3.01943906e-03  4.83345287e-03\n",
      "  1.12190246e-04  9.42468084e-03  7.02128746e-03 -9.85372625e-03\n",
      " -4.43322072e-03 -1.29011157e-03  3.04772262e-03 -4.32395237e-03\n",
      "  1.44916656e-03 -7.84589909e-03  2.77807354e-03  4.70269192e-03\n",
      "  4.93731257e-03 -3.17570218e-03 -8.42704065e-03 -9.22061782e-03\n",
      " -7.22899451e-04 -7.32746487e-03 -6.81496272e-03  6.12000562e-03\n",
      "  7.17230327e-03  2.11741915e-03 -7.89940078e-03 -5.69898821e-03\n",
      "  8.05184525e-03  3.92084382e-03 -5.24047017e-03 -7.39190448e-03\n",
      "  7.71554711e-04  3.46375466e-03  2.07919348e-03  3.10080405e-03\n",
      " -5.62050007e-03 -9.88948625e-03 -7.02083716e-03  2.30308768e-04\n",
      "  4.61867917e-03  4.52630781e-03  1.87981245e-03  5.17067453e-03\n",
      " -1.05360748e-04  4.11416637e-03 -9.12324060e-03  7.70091172e-03\n",
      "  6.14747405e-03  5.12415636e-03  7.20666908e-03  8.43979698e-03\n",
      "  7.38695846e-04 -1.70386070e-03  5.18628338e-04 -9.31678060e-03\n",
      "  8.40621442e-03 -6.37993217e-03  8.42784252e-03 -4.24435502e-03\n",
      "  6.46842702e-04 -9.16416850e-03 -9.55856778e-03 -7.83681031e-03\n",
      " -7.73105631e-03  3.75581993e-04 -7.22646248e-03 -4.95021325e-03\n",
      " -5.27170673e-03 -4.28929785e-03  7.01231137e-03  4.82938997e-03\n",
      "  8.68277065e-03  7.09359162e-03 -5.69440611e-03  7.24079600e-03\n",
      " -9.29490291e-03 -2.58756871e-03 -7.75716640e-03  4.19260142e-03]\n"
     ]
    }
   ],
   "source": [
    "# Skip-gram Model \n",
    "skip_gram_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, \n",
    "sg=1) \n",
    "skip_gram_vector = skip_gram_model.wv['heureux'] \n",
    "print(\"Embedding Skip-gram pour 'heureux':\", skip_gram_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle CBOW (Continuous Bag of Words) prédit le mot cible à partir de son contexte, tandis que le modèle Skip-gram prédit les mots contextuels à partir du mot cible. Le vecteur Skip-gram pour \"heureux\" est similaire à celui de CBOW, mais Skip-gram tend à être plus précis pour capturer des relations rares, car il apprend à partir des mots voisins de manière plus détaillée. Cependant, il est plus lent à entraîner, car il nécessite plus de calculs pour chaque mot cible. Par exemple, pour des mots peu fréquents comme \"satisfaction\", Skip-gram peut mieux capturer les relations contextuelles, mais prendra plus de temps que CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embedding for 'heureux': [-0.22915  -0.65614  -0.44358   0.029588 -0.47647  -0.11944   0.7095\n",
      "  0.21255   0.60578  -0.226     0.053652  0.44703   0.26775  -0.053877\n",
      " -0.13464   0.21553  -0.54511   0.10145  -0.369     0.021499  0.29016\n",
      " -0.17675  -0.72958  -0.51357  -0.088472  0.15428   0.18378   0.10976\n",
      "  0.48002   0.15699  -0.1702   -0.67175  -0.48325  -0.19794   0.52859\n",
      " -0.23422  -0.027806 -0.26104  -0.29968  -0.40698  -0.16376   0.001507\n",
      "  0.15769   0.62273  -0.069709 -0.59361  -0.94486   0.54378  -0.15915\n",
      "  0.52915  -0.39988   0.55962   0.18667  -0.1351   -0.071653  0.56986\n",
      "  0.4166    0.28237  -0.78725  -0.42032   0.017283 -0.58799  -0.65441\n",
      "  0.10836   0.14421   0.24287  -0.56625  -0.56778   0.22266  -0.19948\n",
      " -0.021628  0.31603   0.40921   0.2794    0.27607   0.75458   0.1013\n",
      "  0.40477   0.29041   0.029665 -0.11493  -0.14274   0.12472   0.064805\n",
      "  0.41909  -0.45244   0.15588   0.13639  -0.37713   0.17289  -0.48795\n",
      "  0.24      0.49909  -0.091215  0.57424   0.12799   0.20547   0.22812\n",
      " -0.95232   0.2143  ]\n",
      " Mots similaires à 'word': [('egyptienne', 0.7946601510047913), ('psyché', 0.7924875020980835), ('mutine', 0.7839182019233704), ('topaze', 0.782024085521698), ('warramunga', 0.7792785167694092), ('uranami', 0.7778733372688293), ('curieux', 0.7731819152832031), ('edaphosaurus', 0.7705174088478088), ('sémillante', 0.7680737376213074), ('raisonnable', 0.7665330171585083)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api \n",
    " \n",
    "# Charger les embeddings de GloVe directement de gensim's API  \n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # \"100\" denotes 100-dimensional embeddings \n",
    " \n",
    "# Example: Retrouver l’embedding pour word \n",
    "word = \"heureux\" \n",
    "if word in glove_model: \n",
    "    embedding = glove_model[word] \n",
    "    print(f\"GloVe Embedding for '{word}':\", embedding) \n",
    " \n",
    "# Find similar words \n",
    "similar_words = glove_model.most_similar(\"heureux\") \n",
    "print(f\" Mots similaires à 'word':\", similar_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des tailles des vecteurs :\n",
    "GloVe : Vecteur de dimension 100.\n",
    "Word2Vec (CBOW/Skip-gram) : Vecteur de dimension 100.\n",
    "\n",
    "### Comparaison des embeddings :\n",
    "Les valeurs des vecteurs GloVe sont plus grandes en magnitude (ex. -0.22915 à 0.7095), tandis que celles de Word2Vec (CBOW et Skip-gram) sont plus petites (proches de zéro, ex. 0.0013 à -0.0098).\n",
    "GloVe capture des co-occurrences globales, tandis que Word2Vec est basé sur un contexte local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding FastText pour ''heureux'': [ 1.5102547e-03  1.5297874e-03  5.0949125e-04  1.5477291e-03\n",
      "  6.1322114e-04 -3.3465200e-03  2.1629718e-03  5.0185376e-04\n",
      " -1.5412853e-03 -8.8890287e-04 -7.2054751e-04 -5.9478084e-04\n",
      " -1.1453654e-03  2.2913993e-04 -5.1365923e-03  2.0210396e-03\n",
      "  3.8804200e-03 -2.2704499e-03  2.0007330e-03  2.0561130e-03\n",
      " -1.4081963e-03 -1.1745071e-03 -3.6445227e-03  4.8637716e-04\n",
      " -1.0660794e-03 -6.4542772e-05  1.8681042e-03 -1.7196456e-03\n",
      "  2.6108227e-03 -3.7981363e-04 -3.2549177e-03 -5.2663992e-04\n",
      "  6.2893050e-05  1.1939853e-03 -3.7030116e-04  4.8628813e-04\n",
      "  2.1966894e-03  1.8661494e-03 -1.3891333e-03  1.9020773e-03\n",
      " -9.2450704e-04 -2.0607989e-04  2.7609160e-04 -2.3982310e-04\n",
      " -3.8618266e-03  3.0112448e-03 -1.1309056e-03  3.8316909e-03\n",
      "  2.6609763e-04  5.1402132e-04 -2.8808047e-03 -3.9501907e-03\n",
      " -1.4878053e-03  2.0369259e-03  2.4707885e-03  6.6925318e-04\n",
      "  8.9598243e-04 -7.6899381e-04  1.3666723e-03 -4.2575095e-03\n",
      " -7.2096380e-05  5.9888570e-04  2.5178350e-03 -3.4119755e-03\n",
      " -9.0738853e-05  1.7254126e-03 -2.4004609e-03  7.8467297e-04\n",
      " -4.0223775e-05  2.0799681e-03  5.2438938e-04 -8.5839676e-04\n",
      "  2.4558674e-03 -6.5875205e-04  5.7562295e-04 -1.4693317e-03\n",
      "  9.6613279e-04  8.2040578e-04 -4.0552816e-03 -1.5398492e-03\n",
      "  2.0272383e-03  6.5882414e-05  3.7293218e-03 -4.6632314e-04\n",
      " -8.7655941e-04 -1.3947396e-03 -2.4249237e-03 -7.2189904e-04\n",
      "  2.1393714e-03  2.0708649e-03  1.2730525e-03 -4.6926411e-04\n",
      " -1.7810540e-04 -1.6904098e-03 -9.3457039e-04  2.4648305e-04\n",
      "  2.4361303e-04  2.3243790e-03 -7.5413095e-04 -1.0450373e-03]\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText(sentences=tokenized_docs, vector_size=100, window=3, min_count=1, sg=1, \n",
    "epochs=10) \n",
    "# # Exemple d'embedding pour un mot \n",
    "embedding_fasttext = fasttext_model.wv['word'] \n",
    "print(\"Embedding FastText pour ''heureux'':\", embedding_fasttext) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots similaires à ' heureux ' utilisant FastText: [('heureux', 0.5331311821937561), ('nlp', 0.2674098014831543), ('importants', 0.200381338596344), ('mots', 0.10518591850996017), ('capturent', 0.08989571779966354), ('naturel', 0.047531139105558395), ('ici', 0.008579511195421219), ('traitement', -0.003684221999719739), ('embeddings', -0.02063317596912384), ('sens', -0.02793550118803978)]\n"
     ]
    }
   ],
   "source": [
    "# Trouver les mots similaires à \" heureux \" \n",
    "similar_words_fasttext = fasttext_model.wv.most_similar(' heureux ') \n",
    "print(\"Mots similaires à ' heureux ' utilisant FastText:\", similar_words_fasttext) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText est particulièrement adapté aux langues riches en morphologie et aux mots rares grâce à son utilisation des sous-mots. Cela le rend efficace pour gérer les mots inconnus (out-of-vocabulary, OOV). Cependant, cette méthode demande plus de ressources computationnelles, ce qui peut être un inconvénient dans des environnements contraints. \n",
    "\n",
    "Word2Vec, en revanche, est simple et rapide, mais il ne gère pas les mots inconnus, limitant son efficacité pour les corpus avec des termes peu fréquents ou absents du vocabulaire d’entraînement. Malgré cela, il reste performant pour des corpus bien structurés où les mots rares sont moins problématiques.\n",
    "\n",
    "GloVe excelle dans la capture des relations sémantiques globales grâce à son approche basée sur les cooccurrences. Bien qu’il ne gère pas non plus les mots inconnus, il offre une performance équilibrée avec des besoins computationnels modérés.\n",
    "\n",
    "Ainsi, FastText est recommandé pour des textes variés et des langues complexes, tandis que Word2Vec et GloVe conviennent mieux aux corpus standards, avec une préférence pour GloVe si les relations globales sont essentielles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forces et limites des modèles :\n",
    "    FastText : Précis pour les mots rares et composés, mais plus lent en calcul.\n",
    "    Word2Vec : Rapide avec une précision élevée pour les mots fréquents, mais inefficace pour les mots rares.\n",
    "    GloVe : Captures globalement les relations sémantiques, mais moins performant pour les mots inconnus.\n",
    "\n",
    "2. Pourquoi FastText pour les langues complexes ?\n",
    "FastText utilise des sous-mots, permettant de gérer efficacement les mots rares ou inconnus, essentiels pour les langues morphologiquement riches.\n",
    "\n",
    "3. Différence conceptuelle entre GloVe et Word2Vec :\n",
    "GloVe utilise des cooccurrences globales dans le corpus, tandis que Word2Vec se base sur des contextes locaux des mots (fenêtres d’apparition).\n",
    "\n",
    "4. Cas d’usage de CBOW et Skip-gram :\n",
    "CBOW : Rapide, adapté pour des corpus larges et prédictions générales.\n",
    "Skip-gram : Plus précis pour les mots rares et contextes spécifiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extension du Corpus avec NLTK et Analyse des Temps d'Exécution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('movie_reviews') \n",
    "from nltk.corpus import movie_reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les phrases du corpus \n",
    "corpus = [\" \".join(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_corpus = []\n",
    "    for text in corpus:\n",
    "        tokens = word_tokenize(text.lower())  # Convertir en minuscules et tokeniser\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]  # Suppression stop words et ponctuation\n",
    "        preprocessed_corpus.append(tokens)\n",
    "    return preprocessed_corpus\n",
    "\n",
    "# Préparer le corpus\n",
    "preprocessed_corpus = preprocess_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Entraîner le modèle Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=preprocessed_corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Obtenir l'embedding d'un mot\n",
    "embedding_word2vec = word2vec_model.wv['happy']  # Exemple avec le mot \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22634144  0.36658475  0.38456032  0.33503675 -0.44472966 -0.81369466\n",
      "  0.13317102  0.88921183 -0.06517924 -0.37211624 -0.1187766  -0.9068398\n",
      " -0.07966147  0.0981816   0.2157285  -0.2570504   0.25583804 -0.9008905\n",
      "  0.01104465 -1.0535442   0.2577613   0.26995233  0.8430826  -0.15455064\n",
      " -0.04965321  0.07999973 -0.46574187 -0.35755718 -0.28785053  0.27197433\n",
      "  0.45745715  0.25736237  0.04997671 -0.42251438 -0.02859682  0.47991294\n",
      "  0.5217633  -0.40840212 -0.2420582  -0.6748591   0.2710632  -0.22154191\n",
      " -0.38934365  0.04312353  0.4348115  -0.22503722 -0.17942905  0.12972534\n",
      "  0.07271142  0.80456764  0.22129586 -0.52708393 -0.11539223 -0.18184316\n",
      " -0.24300233  0.17456838  0.29951748 -0.04330263 -0.52410537  0.4701127\n",
      "  0.04840739  0.15137617  0.13131836 -0.1377962  -0.6760656   0.49838474\n",
      "  0.25881925  0.5436593  -0.550566    0.78370047 -0.27625212  0.32381722\n",
      "  0.5323234  -0.01632714  0.10587885  0.4782794   0.22353889  0.02111039\n",
      " -0.32004344  0.10535039 -0.18611637 -0.17227475  0.01784816  0.63823754\n",
      " -0.4371722  -0.3080718   0.21418287  0.29586124  0.30134365  0.46097496\n",
      "  0.7939701   0.15258281 -0.082164    0.2662814   0.8740729   0.52496004\n",
      "  0.14368996 -0.11877284 -0.13528456  0.31179768]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour obtenir l'embedding d'un mot\n",
    "def get_word_embedding(word, model):\n",
    "    return model[word] if word in model else np.zeros(model.vector_size)\n",
    "\n",
    "# Obtenir les embeddings pour chaque phrase\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    embeddings = [get_word_embedding(word, model) for word in sentence]\n",
    "    if embeddings:  # Moyenne des vecteurs pour représenter la phrase\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Générer les embeddings pour le corpus\n",
    "corpus_embeddings = [get_sentence_embedding(sentence, glove_model) for sentence in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots similaires à 'heureux': [('egyptienne', 0.7946601510047913), ('psyché', 0.7924875020980835), ('mutine', 0.7839182019233704), ('topaze', 0.782024085521698), ('warramunga', 0.7792785167694092), ('uranami', 0.7778733372688293), ('curieux', 0.7731819152832031), ('edaphosaurus', 0.7705174088478088), ('sémillante', 0.7680737376213074), ('raisonnable', 0.7665330171585083)]\n"
     ]
    }
   ],
   "source": [
    "word = \"heureux\"\n",
    "if word in glove_model:\n",
    "    similar_words = glove_model.most_similar(word)\n",
    "    print(f\"Mots similaires à '{word}':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Entraîner le modèle FastText\n",
    "fasttext_model = FastText(sentences=preprocessed_corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Obtenir l'embedding d'un mot\n",
    "embedding_fasttext = fasttext_model.wv['happy']  # Exemple avec le mot \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.2328138e-01  9.8402694e-02 -5.8627713e-01  2.1383159e-01\n",
      "  2.9167512e-01  6.9047397e-01 -9.0338761e-01  3.8167211e-01\n",
      "  1.0629005e+00 -8.4670402e-02 -4.3252411e-01  3.5405390e-02\n",
      " -5.3349484e-02  4.6524683e-01  3.4900028e-01  2.2641460e-03\n",
      " -3.4510292e-02 -8.0749822e-01 -3.1985405e-01 -5.9283310e-01\n",
      " -9.0390223e-01 -1.4241166e-01 -5.5265880e-01 -5.1646835e-01\n",
      " -2.8719634e-01 -3.7077728e-01  6.5048449e-02  2.5472134e-01\n",
      "  6.5454072e-01  1.2151333e-01 -1.3821451e-01  9.9874705e-02\n",
      " -4.3321189e-02  7.3669441e-02  1.7232794e-01  4.9178770e-01\n",
      "  4.2714527e-01  1.0523230e+00 -5.5913162e-01 -3.1619892e-01\n",
      "  3.7020452e-02  1.1021802e-01 -3.7913308e-02 -4.8233813e-01\n",
      " -9.6696448e-01 -5.7944852e-01 -4.1664027e-02 -4.6984950e-04\n",
      "  6.1652827e-01  7.3949319e-01  2.9008725e-01  5.1252049e-01\n",
      "  5.9232628e-01  4.5207236e-02  2.3520637e-01 -4.0608945e-01\n",
      " -8.4129596e-01  7.4522778e-02 -2.2772101e-01  3.5830311e-02\n",
      "  3.2047310e-01  8.5679434e-02 -1.1635340e+00  1.0147573e+00\n",
      " -4.7988892e-01  4.9175340e-01  2.8107998e-01  1.8798462e-01\n",
      "  2.1864032e-02  6.2273163e-01 -3.7661444e-02  3.9003924e-01\n",
      "  9.1576827e-01 -4.7817770e-01  2.7474130e-02  1.6280875e-01\n",
      "  3.8544208e-01  1.5869610e-01  1.4082985e-01 -1.3281299e-01\n",
      "  1.0268989e-01  4.0697619e-01 -8.7190729e-01  7.5986817e-02\n",
      " -3.5905239e-01 -1.3169031e-01 -2.1229538e-01 -5.6542054e-02\n",
      "  6.0750943e-02 -8.8168740e-01 -1.3343585e+00 -3.9932767e-01\n",
      " -5.8807200e-01  5.4321114e-02 -2.0487127e-01  8.9490777e-01\n",
      "  1.3136715e-01  5.1276428e-01 -3.4961796e-01  2.1158618e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 :\n",
    "Plus rapide : Word2Vec CBOW, grâce à sa prédiction rapide des mots centraux.\n",
    "Plus lente : FastText, car il génère des embeddings pour les sous-mots, augmentant le temps de calcul.\n",
    "\n",
    "Question 2 :\n",
    "Techniques avancées (Word2Vec, GloVe, FastText) : Le temps augmente avec la taille du corpus, car l'entraînement est plus complexe et itératif.\n",
    "Techniques classiques (One-Hot Encoding, TF-IDF, BoW, N-grams) : L'impact est linéaire, dépendant uniquement de la taille des données et de la mémoire utilisée.\n",
    "\n",
    "Question 3 :\n",
    "Fortement affectées : Word2Vec et FastText, car elles nécessitent plusieurs passes sur le corpus pour ajuster les poids.\n",
    "Raison : Leur entraînement repose sur des itérations (epochs) et des calculs complexes.\n",
    "\n",
    "Question 4 :\n",
    "Différences observées : Avec un grand corpus, les vecteurs sont plus riches et capturent mieux les relations sémantiques. Les embeddings pour les mots rares ou inconnus sont particulièrement améliorés dans FastText.\n",
    "Améliorations : Meilleure généralisation et représentations plus précises.\n",
    "\n",
    "Question 5 :\n",
    "Techniques classiques : Leur temps d'exécution augmente proportionnellement à la taille du corpus, car elles traitent chaque mot ou document séparément.\n",
    "Techniques avancées : Elles nécessitent des ressources importantes initialement (entraînement), mais sont rapides à utiliser une fois pré-entraînées."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
